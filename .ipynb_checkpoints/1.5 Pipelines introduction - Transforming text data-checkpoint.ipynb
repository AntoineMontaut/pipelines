{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372624</th>\n",
       "      <td>1GB Laptop RAM Memory Upgrade for Sony Vaio VP...</td>\n",
       "      <td>Computers/Tablets &amp; Networking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104090</th>\n",
       "      <td>Picasso Jasper Stone Flared Ear Plug 3/4\" 19mm...</td>\n",
       "      <td>Jewellery &amp; Watches</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187196</th>\n",
       "      <td>MENS ZICO DENIM JOGGER JEANS CRUISE IN BLUE WA...</td>\n",
       "      <td>Clothes, Shoes &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509735</th>\n",
       "      <td>Genuine LUCAS Indicator Switch Stalk For MGB, ...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400774</th>\n",
       "      <td>Savannah H2284 Girls Shoes Dark Pink Or Nude/G...</td>\n",
       "      <td>Clothes, Shoes &amp; Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "372624  1GB Laptop RAM Memory Upgrade for Sony Vaio VP...   \n",
       "104090  Picasso Jasper Stone Flared Ear Plug 3/4\" 19mm...   \n",
       "187196  MENS ZICO DENIM JOGGER JEANS CRUISE IN BLUE WA...   \n",
       "509735  Genuine LUCAS Indicator Switch Stalk For MGB, ...   \n",
       "400774  Savannah H2284 Girls Shoes Dark Pink Or Nude/G...   \n",
       "\n",
       "                         category_name  \n",
       "372624  Computers/Tablets & Networking  \n",
       "104090             Jewellery & Watches  \n",
       "187196    Clothes, Shoes & Accessories  \n",
       "509735     Vehicle Parts & Accessories  \n",
       "400774    Clothes, Shoes & Accessories  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ebaytitles.csv\")\n",
    "df = df.sample(frac=0.1) # delete this line if you are brave and have many GBs of RAM\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out unique values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Computers/Tablets & Networking', 'Jewellery & Watches',\n",
       "       'Clothes, Shoes & Accessories', 'Vehicle Parts & Accessories',\n",
       "       'Garden & Patio', 'Sporting Goods', 'Home, Furniture & DIY',\n",
       "       'Health & Beauty', 'Art', 'Crafts', 'Collectibles',\n",
       "       'Mobile Phones & Communication', 'Cameras & Photography', 'Music',\n",
       "       'Business, Office & Industrial', 'Sound & Vision',\n",
       "       'DVDs, Films & TV', 'Toys & Games', 'Baby',\n",
       "       'Musical Instruments & Gear', 'Books, Comics & Magazines',\n",
       "       'Video Games & Consoles', 'Sports Memorabilia', 'Dolls & Bears',\n",
       "       'Pet Supplies', 'Cell Phones & Accessories', 'Coins & Paper Money',\n",
       "       'Everything Else', 'Antiques', 'Wholesale & Job Lots',\n",
       "       'Consumer Electronics', 'Stamps', 'Pottery, Porcelain & Glass',\n",
       "       'Travel', 'Entertainment Memorabilia'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test observations - there is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.title.values\n",
    "y = df.category_name.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "------------------\n",
    "\n",
    "1. Count how many titles are in each category (```pandas.DataFrame.groupby```). Print out most common at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vehicle Parts &amp; Accessories</th>\n",
       "      <td>22960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clothes, Shoes &amp; Accessories</th>\n",
       "      <td>16549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home, Furniture &amp; DIY</th>\n",
       "      <td>13096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computers/Tablets &amp; Networking</th>\n",
       "      <td>6777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewellery &amp; Watches</th>\n",
       "      <td>6319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sporting Goods</th>\n",
       "      <td>4725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mobile Phones &amp; Communication</th>\n",
       "      <td>3905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health &amp; Beauty</th>\n",
       "      <td>3388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crafts</th>\n",
       "      <td>3359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toys &amp; Games</th>\n",
       "      <td>2906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business, Office &amp; Industrial</th>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collectibles</th>\n",
       "      <td>2508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sound &amp; Vision</th>\n",
       "      <td>1942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garden &amp; Patio</th>\n",
       "      <td>1042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cameras &amp; Photography</th>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baby</th>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pet Supplies</th>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVDs, Films &amp; TV</th>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Art</th>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books, Comics &amp; Magazines</th>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video Games &amp; Consoles</th>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musical Instruments &amp; Gear</th>\n",
       "      <td>437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Memorabilia</th>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dolls &amp; Bears</th>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coins &amp; Paper Money</th>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antiques</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Everything Else</th>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Electronics</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wholesale &amp; Job Lots</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pottery, Porcelain &amp; Glass</th>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stamps</th>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell Phones &amp; Accessories</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment Memorabilia</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title\n",
       "category_name                        \n",
       "Vehicle Parts & Accessories     22960\n",
       "Clothes, Shoes & Accessories    16549\n",
       "Home, Furniture & DIY           13096\n",
       "Computers/Tablets & Networking   6777\n",
       "Jewellery & Watches              6319\n",
       "Sporting Goods                   4725\n",
       "Mobile Phones & Communication    3905\n",
       "Health & Beauty                  3388\n",
       "Crafts                           3359\n",
       "Toys & Games                     2906\n",
       "Business, Office & Industrial    2809\n",
       "Collectibles                     2508\n",
       "Sound & Vision                   1942\n",
       "Music                            1339\n",
       "Garden & Patio                   1042\n",
       "Cameras & Photography             971\n",
       "Baby                              719\n",
       "Pet Supplies                      628\n",
       "DVDs, Films & TV                  597\n",
       "Art                               582\n",
       "Books, Comics & Magazines         466\n",
       "Video Games & Consoles            448\n",
       "Musical Instruments & Gear        437\n",
       "Sports Memorabilia                275\n",
       "Dolls & Bears                     258\n",
       "Coins & Paper Money               209\n",
       "Antiques                          207\n",
       "Everything Else                   149\n",
       "Consumer Electronics              121\n",
       "Wholesale & Job Lots               81\n",
       "Pottery, Porcelain & Glass         80\n",
       "Stamps                             79\n",
       "Cell Phones & Accessories          63\n",
       "Entertainment Memorabilia           5\n",
       "Travel                              1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category_name').count().sort_values('title', ascending=False)\n",
    "# df.loc[:,'category_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "print(frequencies)\n",
    "\n",
    "# or faster\n",
    "\n",
    "df.category_name.value_counts()\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaner(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = ' '.join(txt.split()) # great to get rid of tabs, double space, etc.\n",
    "    txt = txt.strip(' ')\n",
    "    txt = txt.lstrip('\\t')\n",
    "    txt= re.sub(\"[^A-Za-z0-9]\",\" \",txt)\n",
    "#     txt = re.sub(\"[0-9]+\",\"#\",txt)\n",
    "    return txt\n",
    "    \n",
    "cleaner('\\ttest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "['0', '00', '000', '0000', '00000m', '00003f', '000051446b', '000051446d', '000067', '00008840511', '0001', '000110', '0001107423', '00013', '000145', '000146', '0002003822', '000210', '000222', '000291', '0003', '00033', '000331', '00034', '00035']\n",
      "71655 \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Count Vectorizer max_df=1\n",
      "['00000m', '00003f', '000051446b', '000051446d', '000067', '00008840511', '000110', '0001107423', '00013', '000146', '0002003822', '000210', '000222', '000291', '00033', '00034', '00035', '000361', '00042', '000500', '00051', '00051446l', '00056', '000589', '000594']\n",
      "42391 \n",
      " ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words=None,\n",
    "#                     token_pattern=r'(?u)\\b\\w\\w+\\b')),\n",
    "    ('Count Vectorizer', \n",
    "     CountVectorizer(stop_words=None,\n",
    "                    token_pattern=r'(?u)\\b\\w+\\b')),\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words='english',\n",
    "#                     token_pattern=r'(?u)\\b\\w\\w+\\b')),\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words='english',\n",
    "#                     token_pattern=r'(?u)\\b\\w+\\b')),\n",
    "#     ('Count Vectorizer min_df=5', \n",
    "#      CountVectorizer(stop_words=None,\n",
    "#                     token_pattern=r'(?u)\\b\\w+\\b',\n",
    "#                     min_df=5)),\n",
    "    ('Count Vectorizer max_df=1', \n",
    "     CountVectorizer(stop_words=None,\n",
    "                    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                    max_df=1)),\n",
    "#     ('Count Vectorizer ngram_range=(1,3)', \n",
    "#      CountVectorizer(preprocessor=cleaner,\n",
    "#                     analyzer='char',\n",
    "#                     ngram_range=(1,3))),\n",
    "#     ('Count Vectorizer ngram_range=(2,4)', \n",
    "#      CountVectorizer(preprocessor=cleaner,\n",
    "#                     analyzer='char',\n",
    "#                     ngram_range=(2,4))),\n",
    "]\n",
    "\n",
    "# ('Hashing Vectorizer',\n",
    "#      HashingVectorizer(token_pattern=r'(?u)\\b\\w+\\b',\n",
    "#                       n_features=100000,\n",
    "#                       stop_words='english'))\n",
    "\n",
    "for name, vect in vectorizers:\n",
    "    print(name)\n",
    "    vect.fit(X_tr)\n",
    "\n",
    "    print(list(vect.get_feature_names())[:25])\n",
    "    print(len(vect.get_feature_names()), '\\n', '-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "------------------\n",
    "\n",
    "Linguistic normalization in which variant forms are reduced to a common form\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "    \n",
    "Usage:\n",
    "\n",
    "    import snowballstemmer\n",
    "\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    print(stemmer.stemWords(\"We are the world\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier #1\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.67      0.19      0.30        31\n",
      "                           Art       0.61      0.69      0.65        55\n",
      "                          Baby       0.81      0.63      0.71        62\n",
      "     Books, Comics & Magazines       0.97      0.67      0.79        43\n",
      " Business, Office & Industrial       0.78      0.63      0.70       287\n",
      "         Cameras & Photography       0.89      0.86      0.88        88\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         8\n",
      "  Clothes, Shoes & Accessories       0.93      0.97      0.95      1684\n",
      "           Coins & Paper Money       0.96      0.89      0.92        27\n",
      "                  Collectibles       0.74      0.68      0.71       248\n",
      "Computers/Tablets & Networking       0.95      0.96      0.96       701\n",
      "          Consumer Electronics       1.00      0.50      0.67        16\n",
      "                        Crafts       0.88      0.84      0.86       320\n",
      "              DVDs, Films & TV       0.86      0.94      0.90        69\n",
      "                 Dolls & Bears       0.89      0.80      0.84        20\n",
      "     Entertainment Memorabilia       0.17      1.00      0.29         1\n",
      "               Everything Else       1.00      0.57      0.73         7\n",
      "                Garden & Patio       0.84      0.72      0.78        90\n",
      "               Health & Beauty       0.84      0.90      0.87       354\n",
      "         Home, Furniture & DIY       0.85      0.89      0.87      1310\n",
      "           Jewellery & Watches       0.95      0.96      0.96       647\n",
      " Mobile Phones & Communication       0.92      0.94      0.93       403\n",
      "                         Music       0.96      0.96      0.96       135\n",
      "    Musical Instruments & Gear       0.95      0.77      0.85        47\n",
      "                  Pet Supplies       0.87      0.87      0.87        60\n",
      "    Pottery, Porcelain & Glass       0.75      0.38      0.50         8\n",
      "                Sound & Vision       0.86      0.72      0.79       199\n",
      "                Sporting Goods       0.88      0.81      0.84       482\n",
      "            Sports Memorabilia       0.69      0.61      0.65        18\n",
      "                        Stamps       1.00      0.90      0.95        10\n",
      "                  Toys & Games       0.87      0.76      0.81       304\n",
      "                        Travel       0.00      0.00      0.00         0\n",
      "   Vehicle Parts & Accessories       0.95      0.97      0.96      2215\n",
      "        Video Games & Consoles       0.81      0.88      0.84        40\n",
      "          Wholesale & Job Lots       0.86      0.55      0.67        11\n",
      "\n",
      "                   avg / total       0.90      0.90      0.90     10000\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier #2\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.78      0.23      0.35        31\n",
      "                           Art       0.62      0.65      0.64        55\n",
      "                          Baby       0.82      0.60      0.69        62\n",
      "     Books, Comics & Magazines       0.88      0.67      0.76        43\n",
      " Business, Office & Industrial       0.77      0.60      0.68       287\n",
      "         Cameras & Photography       0.89      0.84      0.87        88\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         8\n",
      "  Clothes, Shoes & Accessories       0.92      0.97      0.94      1684\n",
      "           Coins & Paper Money       0.96      0.93      0.94        27\n",
      "                  Collectibles       0.73      0.67      0.70       248\n",
      "Computers/Tablets & Networking       0.95      0.95      0.95       701\n",
      "          Consumer Electronics       0.88      0.44      0.58        16\n",
      "                        Crafts       0.85      0.83      0.84       320\n",
      "              DVDs, Films & TV       0.91      0.93      0.92        69\n",
      "                 Dolls & Bears       0.84      0.80      0.82        20\n",
      "     Entertainment Memorabilia       0.17      1.00      0.29         1\n",
      "               Everything Else       1.00      0.43      0.60         7\n",
      "                Garden & Patio       0.84      0.69      0.76        90\n",
      "               Health & Beauty       0.85      0.90      0.87       354\n",
      "         Home, Furniture & DIY       0.83      0.88      0.86      1310\n",
      "           Jewellery & Watches       0.95      0.96      0.96       647\n",
      " Mobile Phones & Communication       0.92      0.94      0.93       403\n",
      "                         Music       0.97      0.95      0.96       135\n",
      "    Musical Instruments & Gear       0.97      0.68      0.80        47\n",
      "                  Pet Supplies       0.85      0.85      0.85        60\n",
      "    Pottery, Porcelain & Glass       0.75      0.38      0.50         8\n",
      "                Sound & Vision       0.86      0.77      0.81       199\n",
      "                Sporting Goods       0.89      0.79      0.83       482\n",
      "            Sports Memorabilia       0.58      0.61      0.59        18\n",
      "                        Stamps       1.00      0.90      0.95        10\n",
      "                  Toys & Games       0.86      0.75      0.80       304\n",
      "                        Travel       0.00      0.00      0.00         0\n",
      "   Vehicle Parts & Accessories       0.95      0.97      0.96      2215\n",
      "        Video Games & Consoles       0.83      0.85      0.84        40\n",
      "          Wholesale & Job Lots       0.75      0.55      0.63        11\n",
      "\n",
      "                   avg / total       0.90      0.89      0.89     10000\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    ('Classifier #1',\n",
    "    make_pipeline(\n",
    "        CountVectorizer(\n",
    "            preprocessor=cleaner,\n",
    "            stop_words='english',\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'\n",
    "        ),\n",
    "        StandardScaler(\n",
    "            with_mean=False,\n",
    "            with_std=False\n",
    "        ),\n",
    "        SGDClassifier()\n",
    "    )),\n",
    "    ('Classifier #2',\n",
    "    make_pipeline(\n",
    "        CountVectorizer(\n",
    "            preprocessor=cleaner,\n",
    "            stop_words='english',\n",
    "            token_pattern=r'(?u)\\b\\w+\\b',\n",
    "            min_df=5\n",
    "        ),\n",
    "        StandardScaler(\n",
    "            with_mean=False,\n",
    "            with_std=False\n",
    "        ),\n",
    "        SGDClassifier()\n",
    "    )),\n",
    "#     ('Classifier #2',\n",
    "#     make_pipeline(\n",
    "#         CountVectorizer(\n",
    "#             preprocessor=cleaner,\n",
    "#             analyzer='char',\n",
    "#             ngram_range=(2,5)\n",
    "#         ),\n",
    "#         StandardScaler(\n",
    "#             with_mean=False,\n",
    "#             with_std=False\n",
    "#         ),\n",
    "#         SGDClassifier()\n",
    "#     )),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    print(name)\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    y_pred = classifier.predict(X_te)\n",
    "\n",
    "    print(classification_report(y_te, y_pred))\n",
    "\n",
    "#     preds = cross_val_predict(classifier, \n",
    "#                               X_tr, \n",
    "#                               y_tr, \n",
    "#                               cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "#     print(classification_report(y_tr, preds))\n",
    "          \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', SGDClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds)\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=5)),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('classifier', SGDClassifier())\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid={\n",
    "        'vect__min_df': [2, 5, 10],\n",
    "        'classifier__alpha': [0.5, 0.1, 0.01]\n",
    "    },\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid.fit(X_tr, y_tr)\n",
    "y_pred = grid.predict(X_te)\n",
    "\n",
    "print(classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, params, n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'model__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful materials\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "2. http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
