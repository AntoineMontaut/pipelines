{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/*@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "}\n",
       "@font-face {\n",
       "\tfont-family: \"Computer Modern\";\n",
       "\tfont-weight: bold;\n",
       "\tfont-style: oblique;\n",
       "\tsrc: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "}*/\n",
       "\n",
       ".navbar-brand, .current_kernel_logo {display:none}\n",
       ".container {\n",
       "    width:80%;    \n",
       "}\n",
       "\n",
       "h1 {\n",
       "\tfont-family: Helvetica, serif;\n",
       "}\n",
       "h4{\n",
       "\tmargin-top:12px;\n",
       "\tmargin-bottom: 3px;\n",
       "   }\n",
       "div.text_cell_render{\n",
       "\tfont-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "\tline-height: 145%;\n",
       "\tfont-size: 100%;\n",
       "\twidth:100%;\n",
       "\tmargin-left:auto;\n",
       "\tmargin-right:auto;\n",
       "}\n",
       ".CodeMirror{\n",
       "\t\tfont-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "}\n",
       ".text_cell_render h5 {\n",
       "\tfont-weight: 300;\n",
       "\tfont-size: 22pt;\n",
       "\t/*color: #4057A1;*/\n",
       "\tfont-style: italic;\n",
       "\tmargin-bottom: .5em;\n",
       "\tmargin-top: 0.5em;\n",
       "\tdisplay: block;\n",
       "}\n",
       "\n",
       ".warning{\n",
       "\tcolor: rgb( 240, 20, 20 )\n",
       "\t}   \n",
       "\n",
       "div.spoiler {\n",
       "\tdisplay: none;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "\tborder: 0;\n",
       "\t/*background-color: #eee;*/\n",
       "\tfont-size: 100%;\n",
       "\tpadding: 1px 2px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import css_from_file\n",
    "css_from_file('style/style.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>274003</th>\n",
       "      <td>Retro Flag - Spain Stripe Premium Faux Leather...</td>\n",
       "      <td>Mobile Phones &amp; Communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565955</th>\n",
       "      <td>TOWING 7 PIN PLUG FOR Talbot Samba MODELS CARA...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70145</th>\n",
       "      <td>Greys Prowla Pop up pears LRG 5pcs Pike fishin...</td>\n",
       "      <td>Sporting Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169285</th>\n",
       "      <td>Motorbike/Motorcycle DID Chain &amp; Sprocket Kit ...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5792</th>\n",
       "      <td>VW AUDI SEAT TFSI AIR BOX TO T.I.P LINK PIPE 0...</td>\n",
       "      <td>Vehicle Parts &amp; Accessories</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "274003  Retro Flag - Spain Stripe Premium Faux Leather...   \n",
       "565955  TOWING 7 PIN PLUG FOR Talbot Samba MODELS CARA...   \n",
       "70145   Greys Prowla Pop up pears LRG 5pcs Pike fishin...   \n",
       "169285  Motorbike/Motorcycle DID Chain & Sprocket Kit ...   \n",
       "5792    VW AUDI SEAT TFSI AIR BOX TO T.I.P LINK PIPE 0...   \n",
       "\n",
       "                        category_name  \n",
       "274003  Mobile Phones & Communication  \n",
       "565955    Vehicle Parts & Accessories  \n",
       "70145                  Sporting Goods  \n",
       "169285    Vehicle Parts & Accessories  \n",
       "5792      Vehicle Parts & Accessories  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/ebaytitles.csv\")\n",
    "df = df.sample(frac=0.1) # delete this line if you are brave and have many GBs of RAM\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out unique values of a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mobile Phones & Communication', 'Vehicle Parts & Accessories',\n",
       "       'Sporting Goods', 'Home, Furniture & DIY',\n",
       "       'Clothes, Shoes & Accessories', 'Toys & Games',\n",
       "       'Jewellery & Watches', 'Sound & Vision', 'Crafts',\n",
       "       'Computers/Tablets & Networking', 'Business, Office & Industrial',\n",
       "       'Garden & Patio', 'Collectibles', 'Music', 'Health & Beauty',\n",
       "       'Cameras & Photography', 'DVDs, Films & TV', 'Pet Supplies',\n",
       "       'Video Games & Consoles', 'Books, Comics & Magazines', 'Art',\n",
       "       'Musical Instruments & Gear', 'Baby', 'Sports Memorabilia',\n",
       "       'Antiques', 'Wholesale & Job Lots', 'Coins & Paper Money',\n",
       "       'Dolls & Bears', 'Pottery, Porcelain & Glass',\n",
       "       'Consumer Electronics', 'Everything Else', 'Stamps',\n",
       "       'Cell Phones & Accessories', 'Entertainment Memorabilia', 'Travel',\n",
       "       'Holidays & Travel'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category_name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test observations - there is a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.title.values\n",
    "y = df.category_name.values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, \n",
    "                                          y,\n",
    "                                          test_size=0.1,\n",
    "                                          random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise \n",
    "------------------\n",
    "\n",
    "1. Count how many titles are in each category (```pandas.DataFrame.groupby```). Print out most common at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Vehicle Parts &amp; Accessories</th>\n",
       "      <td>23116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clothes, Shoes &amp; Accessories</th>\n",
       "      <td>16679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Home, Furniture &amp; DIY</th>\n",
       "      <td>12799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Computers/Tablets &amp; Networking</th>\n",
       "      <td>6793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewellery &amp; Watches</th>\n",
       "      <td>6275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sporting Goods</th>\n",
       "      <td>4763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mobile Phones &amp; Communication</th>\n",
       "      <td>3811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Crafts</th>\n",
       "      <td>3421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health &amp; Beauty</th>\n",
       "      <td>3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toys &amp; Games</th>\n",
       "      <td>3081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business, Office &amp; Industrial</th>\n",
       "      <td>2698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collectibles</th>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sound &amp; Vision</th>\n",
       "      <td>1903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td>1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Garden &amp; Patio</th>\n",
       "      <td>1049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cameras &amp; Photography</th>\n",
       "      <td>863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baby</th>\n",
       "      <td>691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pet Supplies</th>\n",
       "      <td>632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DVDs, Films &amp; TV</th>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Art</th>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Books, Comics &amp; Magazines</th>\n",
       "      <td>469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video Games &amp; Consoles</th>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musical Instruments &amp; Gear</th>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Memorabilia</th>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dolls &amp; Bears</th>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coins &amp; Paper Money</th>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Antiques</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Everything Else</th>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consumer Electronics</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pottery, Porcelain &amp; Glass</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wholesale &amp; Job Lots</th>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stamps</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cell Phones &amp; Accessories</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment Memorabilia</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Holidays &amp; Travel</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title\n",
       "category_name                        \n",
       "Vehicle Parts & Accessories     23116\n",
       "Clothes, Shoes & Accessories    16679\n",
       "Home, Furniture & DIY           12799\n",
       "Computers/Tablets & Networking   6793\n",
       "Jewellery & Watches              6275\n",
       "Sporting Goods                   4763\n",
       "Mobile Phones & Communication    3811\n",
       "Crafts                           3421\n",
       "Health & Beauty                  3334\n",
       "Toys & Games                     3081\n",
       "Business, Office & Industrial    2698\n",
       "Collectibles                     2516\n",
       "Sound & Vision                   1903\n",
       "Music                            1380\n",
       "Garden & Patio                   1049\n",
       "Cameras & Photography             863\n",
       "Baby                              691\n",
       "Pet Supplies                      632\n",
       "DVDs, Films & TV                  611\n",
       "Art                               605\n",
       "Books, Comics & Magazines         469\n",
       "Video Games & Consoles            467\n",
       "Musical Instruments & Gear        440\n",
       "Sports Memorabilia                312\n",
       "Dolls & Bears                     289\n",
       "Coins & Paper Money               207\n",
       "Antiques                          177\n",
       "Everything Else                   176\n",
       "Consumer Electronics              113\n",
       "Pottery, Porcelain & Glass         91\n",
       "Wholesale & Job Lots               83\n",
       "Stamps                             82\n",
       "Cell Phones & Accessories          61\n",
       "Entertainment Memorabilia          10\n",
       "Holidays & Travel                   2\n",
       "Travel                              1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('category_name').count().sort_values('title', ascending=False)\n",
    "# df.loc[:,'category_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "frequencies = df.groupby(\"category_name\")[\"title\"].count()\n",
    "frequencies.sort_values(inplace=True,ascending=False)\n",
    "print(frequencies)\n",
    "\n",
    "# or faster\n",
    "\n",
    "df.category_name.value_counts()\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words\n",
    "--------------------\n",
    "\n",
    "Different types of vectorizers:\n",
    "\n",
    "<ul>\n",
    "<li>```sklearn.feature_extraction.text.CountVectorizer``` - Counts the number of times a word appears in the text</li>\n",
    "<li>```sklearn.feature_extraction.text.TfidfVectorizer``` - Weighs the words according to the importance of the word in the context of whole collection. Is the word ```the``` important if it appears in all documents?</li>\n",
    "<li>```sklearn.feature_extraction.text.HashingVectorizer``` - Useful when you don't know the vocabulary upfront. Feature number is calculated as ```hash(token) % vocabulary_size```.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise\n",
    "-------------------\n",
    "1. Use ```CountVectorizer``` / ```TfidfVectorizer``` to fit the collection of documents\n",
    "2. How many unique tokens are there in text? Print some examples (ie first few hundred).\n",
    "3. What methods you can use to reduce this number? \n",
    "   - Check out and experiment with the arguments: ```ngram_range```, ```min_df```. How the vocabulary size changes with each change?\n",
    "   - What would you replace / delete from the text?\n",
    "4. Write a custom function `clean_text` that accepts a text as input and transforms it (remove/hash numbers, delete short/long words etc.)\n",
    "5. (Extra points) When would you use ```HashingVectorizer```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaner(txt):\n",
    "    txt = txt.lower()\n",
    "    txt = ' '.join(txt.split()) # great to get rid of tabs, double space, etc.\n",
    "    txt = txt.strip(' ')\n",
    "    txt = txt.lstrip('\\t')\n",
    "    txt= re.sub(\"[^A-Za-z0-9]\",\" \",txt)\n",
    "#     txt = re.sub(\"[0-9]+\",\"#\",txt)\n",
    "    return txt\n",
    "    \n",
    "cleaner('\\ttest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer\n",
      "['0', '00', '000', '0000', '00000', '00001', '00003', '00003g', '000051446b', '00005320c2', '000060', '000091', '0001', '000106', '00012', '000122', '000146', '000150', '000171528', '0002', '000211', '000223', '000228', '000237', '00027']\n",
      "72268 \n",
      " ----------------------------------------------------------------------------------------------------\n",
      "Count Vectorizer max_df=1\n",
      "['00000', '00001', '00003', '00003g', '000051446b', '00005320c2', '000060', '000091', '0001', '00012', '000122', '000146', '000150', '000171528', '0002', '000211', '000223', '000237', '00027', '000348', '00036a', '00037', '000379', '00038', '000393']\n",
      "43112 \n",
      " ----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words=None,\n",
    "#                     token_pattern=r'(?u)\\b\\w\\w+\\b')),\n",
    "    ('Count Vectorizer', \n",
    "     CountVectorizer(stop_words=None,\n",
    "                    token_pattern=r'(?u)\\b\\w+\\b')),\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words='english',\n",
    "#                     token_pattern=r'(?u)\\b\\w\\w+\\b')),\n",
    "#     ('Count Vectorizer', \n",
    "#      CountVectorizer(stop_words='english',\n",
    "#                     token_pattern=r'(?u)\\b\\w+\\b')),\n",
    "#     ('Count Vectorizer min_df=5', \n",
    "#      CountVectorizer(stop_words=None,\n",
    "#                     token_pattern=r'(?u)\\b\\w+\\b',\n",
    "#                     min_df=5)),\n",
    "    ('Count Vectorizer max_df=1', \n",
    "     CountVectorizer(stop_words=None,\n",
    "                    token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                    max_df=1)),\n",
    "#     ('Count Vectorizer ngram_range=(1,3)', \n",
    "#      CountVectorizer(preprocessor=cleaner,\n",
    "#                     analyzer='char',\n",
    "#                     ngram_range=(1,3))),\n",
    "#     ('Count Vectorizer ngram_range=(2,4)', \n",
    "#      CountVectorizer(preprocessor=cleaner,\n",
    "#                     analyzer='char',\n",
    "#                     ngram_range=(2,4))),\n",
    "]\n",
    "\n",
    "# ('Hashing Vectorizer',\n",
    "#      HashingVectorizer(token_pattern=r'(?u)\\b\\w+\\b',\n",
    "#                       n_features=100000,\n",
    "#                       stop_words='english'))\n",
    "\n",
    "for name, vect in vectorizers:\n",
    "    print(name)\n",
    "    vect.fit(X_tr)\n",
    "\n",
    "    print(list(vect.get_feature_names())[:25])\n",
    "    print(len(vect.get_feature_names()), '\\n', '-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "def clean_text(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(\"[^A-Za-z0-9]\",\" \",t)\n",
    "    t = re.sub(\"[0-9]+\",\"#\",t)\n",
    "    return t\n",
    "\n",
    "vectorizers = [\n",
    "     (\"vanilla\",\n",
    "          CountVectorizer())\n",
    "    ,(\"preprocessing\",\n",
    "          CountVectorizer(preprocessor=clean_text))\n",
    "    ,(\"preprocessing + min_df=10\",\n",
    "          CountVectorizer(preprocessor=clean_text,\n",
    "                          min_df=10))\n",
    "]\n",
    "\n",
    "for vect_name, vect in vectorizers:\n",
    "    print(vect_name)\n",
    "    vect.fit(X_tr)\n",
    "    \n",
    "    print(list(vect.get_feature_names())[:10])\n",
    "    print(len(vect.get_feature_names()))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming\n",
    "------------------\n",
    "\n",
    "Linguistic normalization in which variant forms are reduced to a common form\n",
    "\n",
    "    connection\n",
    "    connections\n",
    "    connective     --->   connect\n",
    "    connected\n",
    "    connecting\n",
    "    \n",
    "Usage:\n",
    "\n",
    "    import snowballstemmer\n",
    "\n",
    "    stemmer = snowballstemmer.stemmer('english')\n",
    "    print(stemmer.stemWords(\"We are the world\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it into a pipeline\n",
    "----------------------\n",
    "\n",
    "Now that we know how to transform text data, let's put it into a pipeline.\n",
    "\n",
    "1. Create a pipeline with `CountVectorizer`, `StandardScaler` and `SGDClassifier` as your final algorithm\n",
    "    a) use alternative format for pipeline definition when you name the steps - refer to the documentation how to do this\n",
    "2. Using ```sklearn.metrics.classification_report``` create a report about your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier #1\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.67      0.40      0.50        15\n",
      "                           Art       0.72      0.74      0.73        58\n",
      "                          Baby       0.87      0.72      0.79        67\n",
      "     Books, Comics & Magazines       0.72      0.76      0.74        41\n",
      " Business, Office & Industrial       0.79      0.62      0.69       280\n",
      "         Cameras & Photography       0.97      0.77      0.86        94\n",
      "     Cell Phones & Accessories       1.00      0.17      0.29         6\n",
      "  Clothes, Shoes & Accessories       0.92      0.97      0.95      1640\n",
      "           Coins & Paper Money       0.89      0.85      0.87        20\n",
      "                  Collectibles       0.85      0.71      0.78       272\n",
      "Computers/Tablets & Networking       0.95      0.95      0.95       659\n",
      "          Consumer Electronics       0.80      0.40      0.53        10\n",
      "                        Crafts       0.89      0.83      0.86       337\n",
      "              DVDs, Films & TV       0.91      0.89      0.90        56\n",
      "                 Dolls & Bears       0.97      0.85      0.91        41\n",
      "               Everything Else       0.91      0.56      0.69        18\n",
      "                Garden & Patio       0.83      0.68      0.75       106\n",
      "               Health & Beauty       0.81      0.89      0.85       304\n",
      "             Holidays & Travel       0.00      0.00      0.00         0\n",
      "         Home, Furniture & DIY       0.85      0.90      0.88      1303\n",
      "           Jewellery & Watches       0.96      0.97      0.96       647\n",
      " Mobile Phones & Communication       0.90      0.93      0.91       355\n",
      "                         Music       0.96      0.93      0.95       139\n",
      "    Musical Instruments & Gear       0.83      0.51      0.63        37\n",
      "                  Pet Supplies       0.86      0.82      0.84        61\n",
      "    Pottery, Porcelain & Glass       0.67      0.22      0.33         9\n",
      "                Sound & Vision       0.79      0.75      0.77       198\n",
      "                Sporting Goods       0.85      0.77      0.81       471\n",
      "            Sports Memorabilia       0.79      0.44      0.56        25\n",
      "                        Stamps       1.00      0.50      0.67         6\n",
      "                  Toys & Games       0.85      0.81      0.83       319\n",
      "                        Travel       0.00      0.00      0.00         0\n",
      "   Vehicle Parts & Accessories       0.95      0.97      0.96      2348\n",
      "        Video Games & Consoles       0.95      0.85      0.90        48\n",
      "          Wholesale & Job Lots       1.00      0.50      0.67        10\n",
      "\n",
      "                   avg / total       0.90      0.90      0.90     10000\n",
      "\n",
      "--------------------------------------------------\n",
      "Classifier #2\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.60      0.40      0.48        15\n",
      "                           Art       0.73      0.62      0.67        58\n",
      "                          Baby       0.83      0.72      0.77        67\n",
      "     Books, Comics & Magazines       0.67      0.76      0.71        41\n",
      " Business, Office & Industrial       0.74      0.58      0.65       280\n",
      "         Cameras & Photography       0.95      0.79      0.86        94\n",
      "     Cell Phones & Accessories       1.00      0.17      0.29         6\n",
      "  Clothes, Shoes & Accessories       0.91      0.97      0.94      1640\n",
      "           Coins & Paper Money       0.94      0.85      0.89        20\n",
      "                  Collectibles       0.84      0.71      0.77       272\n",
      "Computers/Tablets & Networking       0.94      0.94      0.94       659\n",
      "          Consumer Electronics       1.00      0.30      0.46        10\n",
      "                        Crafts       0.88      0.82      0.85       337\n",
      "              DVDs, Films & TV       0.89      0.89      0.89        56\n",
      "                 Dolls & Bears       0.95      0.85      0.90        41\n",
      "               Everything Else       0.91      0.56      0.69        18\n",
      "                Garden & Patio       0.82      0.66      0.73       106\n",
      "               Health & Beauty       0.81      0.89      0.85       304\n",
      "             Holidays & Travel       0.00      0.00      0.00         0\n",
      "         Home, Furniture & DIY       0.86      0.89      0.88      1303\n",
      "           Jewellery & Watches       0.95      0.96      0.96       647\n",
      " Mobile Phones & Communication       0.91      0.91      0.91       355\n",
      "                         Music       0.98      0.93      0.95       139\n",
      "    Musical Instruments & Gear       0.80      0.43      0.56        37\n",
      "                  Pet Supplies       0.80      0.87      0.83        61\n",
      "    Pottery, Porcelain & Glass       0.60      0.33      0.43         9\n",
      "                Sound & Vision       0.79      0.75      0.77       198\n",
      "                Sporting Goods       0.85      0.75      0.80       471\n",
      "            Sports Memorabilia       0.75      0.48      0.59        25\n",
      "                        Stamps       1.00      0.50      0.67         6\n",
      "                  Toys & Games       0.83      0.81      0.82       319\n",
      "                        Travel       0.00      0.00      0.00         0\n",
      "   Vehicle Parts & Accessories       0.94      0.97      0.96      2348\n",
      "        Video Games & Consoles       0.95      0.88      0.91        48\n",
      "          Wholesale & Job Lots       1.00      0.50      0.67        10\n",
      "\n",
      "                   avg / total       0.89      0.89      0.89     10000\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    ('Classifier #1',\n",
    "    make_pipeline(\n",
    "        CountVectorizer(\n",
    "            preprocessor=cleaner,\n",
    "            stop_words='english',\n",
    "            token_pattern=r'(?u)\\b\\w+\\b'\n",
    "        ),\n",
    "        StandardScaler(\n",
    "            with_mean=False,\n",
    "            with_std=False\n",
    "        ),\n",
    "        SGDClassifier()\n",
    "    )),\n",
    "    ('Classifier #2',\n",
    "    make_pipeline(\n",
    "        CountVectorizer(\n",
    "            preprocessor=cleaner,\n",
    "            stop_words='english',\n",
    "            token_pattern=r'(?u)\\b\\w+\\b',\n",
    "            min_df=5\n",
    "        ),\n",
    "        StandardScaler(\n",
    "            with_mean=False,\n",
    "            with_std=False\n",
    "        ),\n",
    "        SGDClassifier()\n",
    "    )),\n",
    "#     ('Classifier #2',\n",
    "#     make_pipeline(\n",
    "#         CountVectorizer(\n",
    "#             preprocessor=cleaner,\n",
    "#             analyzer='char',\n",
    "#             ngram_range=(2,5)\n",
    "#         ),\n",
    "#         StandardScaler(\n",
    "#             with_mean=False,\n",
    "#             with_std=False\n",
    "#         ),\n",
    "#         SGDClassifier()\n",
    "#     )),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    print(name)\n",
    "    classifier.fit(X_tr, y_tr)\n",
    "    y_pred = classifier.predict(X_te)\n",
    "\n",
    "    print(classification_report(y_te, y_pred))\n",
    "\n",
    "#     preds = cross_val_predict(classifier, \n",
    "#                               X_tr, \n",
    "#                               y_tr, \n",
    "#                               cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "#     print(classification_report(y_tr, preds))\n",
    "          \n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer(min_df=10, preprocessor=clean_text)),\n",
    "                ('scaling', StandardScaler(with_mean=False)),\n",
    "                ('clf', SGDClassifier())])\n",
    "\n",
    "preds = cross_val_predict(clf, \n",
    "                          X_tr, \n",
    "                          y_tr, \n",
    "                          cv=8, n_jobs=-1, verbose=True)\n",
    "\n",
    "print(classification_report(y_tr, preds)\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search\n",
    "--------------------------\n",
    "\n",
    "Scikit-learn has `GridSearchCV` and `RandomizedSearchCV`. Both have the same functionality and can be used to find good parameters for the models. What is great about both these classes that they are both transformers - they return an estimator so you can chain them and put in your pipeline.\n",
    "\n",
    "**GridSearchCV** - you specify the exact values of the parameters you want to test\n",
    "**RandomizedSearchCV** - you specify ranges of parameters\n",
    "\n",
    "Exercise\n",
    "----------------------\n",
    "\n",
    "1. Use `GridSearchCV` or `RandomizedSearchCV` to find the best parameters for the models. Check at least 2 parameters.\n",
    "\n",
    "2. Inspect the attribute `cv_results_` after fitting. It gives a nice representation of the learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "                      Antiques       0.33      0.20      0.25        15\n",
      "                           Art       0.50      0.24      0.33        58\n",
      "                          Baby       0.61      0.21      0.31        67\n",
      "     Books, Comics & Magazines       0.24      0.10      0.14        41\n",
      " Business, Office & Industrial       0.56      0.32      0.40       280\n",
      "         Cameras & Photography       0.76      0.50      0.60        94\n",
      "     Cell Phones & Accessories       0.00      0.00      0.00         6\n",
      "  Clothes, Shoes & Accessories       0.67      0.39      0.49      1640\n",
      "           Coins & Paper Money       0.58      0.55      0.56        20\n",
      "                  Collectibles       0.54      0.29      0.37       272\n",
      "Computers/Tablets & Networking       0.76      0.41      0.53       659\n",
      "          Consumer Electronics       0.50      0.10      0.17        10\n",
      "                        Crafts       0.70      0.40      0.51       337\n",
      "              DVDs, Films & TV       0.24      0.07      0.11        56\n",
      "                 Dolls & Bears       0.59      0.32      0.41        41\n",
      "               Everything Else       0.78      0.39      0.52        18\n",
      "                Garden & Patio       0.56      0.42      0.48       106\n",
      "               Health & Beauty       0.67      0.52      0.59       304\n",
      "         Home, Furniture & DIY       0.61      0.38      0.47      1303\n",
      "           Jewellery & Watches       0.72      0.32      0.44       647\n",
      " Mobile Phones & Communication       0.67      0.28      0.39       355\n",
      "                         Music       0.48      0.20      0.28       139\n",
      "    Musical Instruments & Gear       0.44      0.22      0.29        37\n",
      "                  Pet Supplies       0.72      0.30      0.42        61\n",
      "    Pottery, Porcelain & Glass       0.20      0.11      0.14         9\n",
      "                Sound & Vision       0.60      0.29      0.39       198\n",
      "                Sporting Goods       0.64      0.50      0.56       471\n",
      "            Sports Memorabilia       0.38      0.32      0.35        25\n",
      "                        Stamps       0.50      0.33      0.40         6\n",
      "                  Toys & Games       0.58      0.37      0.45       319\n",
      "                        Travel       0.00      0.00      0.00         0\n",
      "   Vehicle Parts & Accessories       0.39      0.93      0.55      2348\n",
      "        Video Games & Consoles       0.34      0.21      0.26        48\n",
      "          Wholesale & Job Lots       1.00      0.30      0.46        10\n",
      "\n",
      "                   avg / total       0.58      0.50      0.48     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=5)),\n",
    "    ('scaling', StandardScaler(with_mean=False)),\n",
    "    ('classifier', SGDClassifier())\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid={\n",
    "#         'vect__min_df': [5, 10],\n",
    "        'vect__max_df': [10, 20],\n",
    "        'classifier__alpha': [0.5, 0.1, 0.01]\n",
    "    },\n",
    "    n_jobs=-1)\n",
    "\n",
    "grid.fit(X_tr, y_tr)\n",
    "y_pred = grid.predict(X_te)\n",
    "\n",
    "print(classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=20, max_features=None, min_df=5,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_...='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False))])\n",
      "{'classifier__alpha': 0.1, 'vect__max_df': 20}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>Double click to show the solution</a>\n",
    "<div class='spoiler'>\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "print(\"Grid search\")\n",
    "print()\n",
    "\n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'vect__binary': [True, False]}\n",
    "\n",
    "grid_clf = GridSearchCV(clf, params, n_jobs=1, verbose=True)\n",
    "grid_clf.fit(X_tr, y_tr)\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params) \n",
    "    \n",
    "print(\"Randomized search\")\n",
    "print()\n",
    "    \n",
    "params = {'vect__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "          'vect__analyzer': [\"word\",\"char\"],\n",
    "          'model__lr__dimensions': [100, 200]}\n",
    "\n",
    "grid_clf = RandomizedSearchCV(clf, params, n_jobs=1, verbose=True, n_iter=8)\n",
    "grid_clf.fit(np.array(X_tr[:10000]), y_tr[:10000])\n",
    "\n",
    "best_params = sorted(grid_clf.grid_scores_, key=lambda x: -x[1])\n",
    "\n",
    "for params, score, _ in best_params:\n",
    "    print(score, params)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful materials\n",
    "\n",
    "1. http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "2. http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
